# Task 2.8: Process Command Implementation

**Phase**: 2 (Core Functionality)  
**Complexity**: Large  
**Status**: Not Started

## Overview

Implement the `process` command to parse M3U playlists, classify content, and store entries in the database. This command consolidates the parsing functionality from the `parse` command with database persistence and removes the duplication between `parse` and `process` commands.

### Success Criteria

- Parse M3U playlists and extract all metadata
- Classify content into movies, tvshows, channels, or uncategorized
- Store processed entries in the database with proper relationships
- Handle duplicates gracefully (based on line hash)
- Apply filters (include/exclude patterns) from config and runtime filters
- Track processing state and logs for audit trail
- Provide detailed statistics and progress feedback
- Complete processing of 10,000+ entries without errors

### Users and Usage

- CLI users processing M3U playlists locally
- Automated workflows processing playlists on schedule
- Development teams testing and validating M3U content

## Technical Approach

### Architecture

The `process` command orchestrates several existing components:

1. **Parser** (`internal/parser`): Read and parse M3U file line by line
2. **Classifier** (`internal/classifier`): Classify content type and extract metadata
3. **Filter** (`internal/filter`): Apply include/exclude patterns
4. **Database** (`internal/database`): Store processed entries with GORM
5. **Logger** (`internal/logger`): Track progress and errors

### Data Flow

```
M3U File → Parser → Filter → Classifier → Database
                                            ↓
                                    processed_lines table
                                    movies/tvshows/channels/uncategorized tables
                                    processing_logs table
```

### Key Components

**ProcessedLine Model** (already exists in `internal/models/playlist_item.go`):
- Stores M3U line metadata with hash-based deduplication
- Polymorphic relationships to movies, tvshows, channels, uncategorized
- Tracks processing state and download status

**Database Operations**:
- Use GORM transactions for batch inserts
- Check for existing line_hash before insert (skip duplicates)
- Create associated movie/tvshow/channel/uncategorized records
- Update processing_logs table with statistics

**Filtering Logic**:
- Load runtime filters from database (`filter_configs` table)
- Apply file-based filters from config if no runtime filters
- Skip entries that don't match include patterns or match exclude patterns

### Major Technical Decisions

1. **Remove `parse` command**: Consolidate functionality into `process` with a `--no-db` flag for database-less validation (or users can use `dryrun` instead)
2. **Batch processing**: Process entries in batches (configurable, default 100) to improve performance
3. **Duplicate handling**: Skip entries with existing line_hash, optionally override with `--force` flag
4. **Transaction safety**: Use database transactions per batch to ensure consistency
5. **Progress reporting**: Show progress every N entries (configurable) for long-running operations

## Implementation Plan

### Phase 1: Core Processing Logic (Medium)

**Task 1.1: Create Processing Service** (Small)
- Create `internal/processor/processor.go` with `Processor` struct
- Initialize with dependencies: parser, classifier, filter, database
- Define `Process(filePath string, opts ProcessOptions)` method
- Define `ProcessOptions` struct: limit, force, batchSize, progressInterval
- Add unit test structure

**Task 1.2: Implement Entry Processing** (Medium)
- Parse M3U file using existing parser
- For each entry:
  - Check if line_hash exists in database (skip if found unless `--force`)
  - Apply filters (skip if doesn't match)
  - Classify content type using classifier
  - Extract season/episode/resolution metadata
  - Create ProcessedLine record with associations
- Return processing statistics

**Task 1.3: Database Integration** (Medium)
- Implement batch inserts with GORM transactions
- Create helper methods:
  - `saveProcessedLine(line *models.ProcessedLine) error`
  - `saveBatch(lines []*models.ProcessedLine) error`
  - `checkDuplicate(lineHash string) (bool, error)`
- Handle foreign key relationships (movie/tvshow/channel/uncategorized)
- Add error handling and rollback on failure

### Phase 2: Processing Logs and Statistics (Small)

**Task 2.1: Processing Log Tracking** (Small)
- Create processing_log entry at start (`in_progress` status)
- Update entry on completion with statistics
- Track: total_processed, duplicates_skipped, filtered_out, errors
- Store error messages if processing fails
- Add timestamps: started_at, completed_at

**Task 2.2: Statistics and Reporting** (Small)
- Collect statistics during processing:
  - Total lines parsed
  - Successfully stored
  - Duplicates skipped
  - Filtered out
  - Classification breakdown (movies/tvshows/channels/uncategorized)
  - Processing duration
- Format and display statistics at completion
- Add verbose mode showing sample entries

### Phase 3: CLI Integration and Command Removal (Small)

**Task 3.1: Update `process` Command** (Small)
- Wire up processor service in `cmd/main.go`
- Implement command logic:
  - Load configuration
  - Initialize database
  - Create processor with dependencies
  - Call processor.Process() with options
  - Display statistics
- Add flags:
  - `--force`: Re-process existing entries
  - `--limit`: Maximum number of items to process
  - `--batch-size`: Batch size for database inserts (default: 100)
  - `--progress`: Show progress every N entries (default: 1000)

**Task 3.2: Remove `parse` Command** (Small)
- Remove `parseCmd` from `cmd/main.go`
- Update help text and documentation
- Update completion notes in task 2.6
- Recommend using `dryrun` for validation without database

**Task 3.3: Update Documentation** (Small)
- Update README.md with `process` command usage
- Add examples for common workflows
- Document flags and options
- Update DEVELOPMENT.md with processing workflow

### Phase 4: Testing and Validation (Medium)

**Task 4.1: Unit Tests** (Medium)
- Test processor with mock dependencies
- Test batch processing logic
- Test duplicate detection
- Test filtering integration
- Test error handling and rollback
- Test statistics collection

**Task 4.2: Integration Tests** (Medium)
- Test full processing workflow with real database
- Test with sample M3U files (100, 1000, 10000 entries)
- Verify database state after processing
- Test `--force` flag behavior
- Test `--limit` flag behavior
- Test with various filter configurations

**Task 4.3: Performance Testing** (Small)
- Benchmark processing speed (entries per second)
- Test memory usage with large files (100k+ entries)
- Verify batch processing improves performance
- Profile and optimize bottlenecks

## Considerations

### Assumptions

- Parser, classifier, and filter components are already implemented and tested
- Database schema includes all necessary tables and relationships
- GORM is configured correctly for batch operations
- Filter system (file-based and runtime) is functional

### Constraints

- Memory usage must be reasonable for large M3U files (streaming parser)
- Processing speed should handle 10,000+ entries in reasonable time (<5 minutes)
- Database transactions should not lock tables for extended periods
- Progress reporting should not significantly impact performance

### Risks and Mitigation

**Risk 1: Memory exhaustion with large files**
- Mitigation: Use streaming parser, process in batches
- Fallback: Add configurable memory limits and batch size

**Risk 2: Database deadlocks during concurrent processing**
- Mitigation: Use appropriate transaction isolation levels
- Fallback: Implement retry logic with exponential backoff

**Risk 3: Partial processing failures**
- Mitigation: Use transactions per batch, track processing state
- Fallback: Add resume capability from last successful batch

**Risk 4: Performance degradation with filters**
- Mitigation: Optimize regex compilation, use efficient data structures
- Fallback: Add filter caching and precompilation

## Not Included

The following features are out of scope for this task:

- TMDB enrichment (handled by Task 2.7)
- Radarr/Sonarr integration (handled by Task 3.2)
- Download functionality (handled by Task 4.1)
- Web UI for processing management
- Real-time progress via WebSocket
- Scheduled/automated processing (future task)
- Multi-file processing in single command
- Parallel processing of multiple files

These features may be added in future iterations.

## Dependencies

- Task 2.1: M3U Parser (completed)
- Task 2.2: Content Classification (completed)
- Task 2.3: Filter System (completed)
- Task 1.3: Database Schema (completed)
- Task 2.6: CLI Structure (completed)

## Acceptance Criteria

- [ ] Process command successfully parses M3U files
- [ ] Content is classified correctly (movies/tvshows/channels/uncategorized)
- [ ] Entries are stored in database with proper relationships
- [ ] Duplicates are detected and skipped based on line_hash
- [ ] `--force` flag allows re-processing of existing entries
- [ ] `--limit` flag restricts processing to N entries
- [ ] Filters (file-based and runtime) are applied correctly
- [ ] Processing logs are created and updated with statistics
- [ ] Statistics are displayed at completion
- [ ] `parse` command is removed from CLI
- [ ] Unit tests achieve >80% code coverage
- [ ] Integration tests pass with sample M3U files
- [ ] Performance: 10,000 entries processed in <5 minutes
- [ ] Memory usage stays reasonable (<500MB for 100k entries)
- [ ] Documentation is updated with examples

## Implementation Notes

### Database Transaction Pattern

```go
// Batch processing with transaction
func (p *Processor) saveBatch(lines []*models.ProcessedLine) error {
    return database.Get().Transaction(func(tx *gorm.DB) error {
        for _, line := range lines {
            if err := tx.Create(line).Error; err != nil {
                return err
            }
        }
        return nil
    })
}
```

### Progress Reporting Pattern

```go
// Show progress every N entries
processed := 0
for i, entry := range entries {
    // Process entry
    processed++
    
    if processed % opts.ProgressInterval == 0 {
        log.Info(fmt.Sprintf("Processed %d/%d entries", processed, total))
    }
}
```

### Filter Integration Pattern

```go
// Apply filters before classification
if !filter.ShouldProcess(entry.GroupTitle, entry.TvgName) {
    stats.FilteredOut++
    continue
}
```
